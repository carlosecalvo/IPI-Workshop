---
title: "IPI Workshop - Building a model with parsnip"
author: "Carlos Calvo Hernandez"
date: "5/1/2019"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(cli)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

# Tidymodels: `parsnip`[^1]

[^1]: Based on the `parsnip` tidymodels github vignettes.

## What is `parsnip`?

`parsnip` provides functions and methods to create and manipulate functions commonly used during modeling (e.g. fitting the model, making predictions, etc). It allows the user to manipulate how the same type of model can be created from different sources. It also contains a basic framework for model parameter tuning.

## Motivation

Modeling functions across different R packages can have very different interfaces. If you would like to try different approaches, there is a lot of syntactical minutiae to remember. The problem worsens when we move in-between platforms (e.g. doing a logistic regression in R's `glm` versus Spark's implementation). `parsnip` tries to solve this by providing similar interfaces to models.

The idea of `parsnip` is to:

* Separate the definition of a model from its evaluation.
* Decouple the model specification from the implementation (whether the implementation is in R, spark, or something else). For example, the user would call `rand_forest` instead of `ranger::ranger` or other specific packages. 
* Harmonize the argument names (e.g. `n.trees`, `ntrees`, `trees`) so that users can remember a single name. This will help _across_ model types too so that `trees` will be the same argument across random forest as well as boosting or bagging.

Here I'll reuse an example I've been using, if we are fitting a random forest model and would like to adjust the number of trees in the forest there are different argument names to remember:

* `randomForest::randomForest` uses `ntree`,
* `ranger::ranger` uses `num.trees`,  
* Spark's `sparklyr::ml_random_forest` uses `num_trees`.

Rather than remembering these different names, we can create a model through a common interface to these models:

```{r rf-ex, eval=FALSE}
library(parsnip)
rf_mod <- rand_forest(trees = 2000)
```

This way the package makes the translation between `trees` and the real names in each of the implementations.

Some terminology:

* The **model type** differentiates models. Example types are: random forests, logistic regression, linear support vector machines, etc. 
* The **mode** of the model denotes how it will be used. Two common modes are _classification_ and _regression_. Others would include "censored regression" and "risk regression" (parametric and Cox PH models for censored data, respectively), as well as unsupervised models (e.g. "clustering"). 
* The **computational engine** indicates how the actual model might be fit. These are often R packages (such as `randomForest` or `ranger`) but might also be methods outside of R (e.g. Stan, Spark, and others). 

`parsnip`, similar to `ggplot2`, `dplyr` and `recipes`, separates the specification of what you want to do from the actual doing. This allows us to create broader functionality for modeling. 

## Specifying Arguments

Commonly used arguments to the modeling functions have their parameters exposed in the function. For example, `rand_forest` has arguments for:

* `mtry`: The number of predictors that will be randomly sampled at each split when creating the tree models.
* `trees`: The number of trees contained in the ensemble.
* `min_n`: The minimum number of data points in a node that are required for the node to be split further.

The arguments to the default function are:

```{r rf-def}
args(rand_forest)
```

However, there might be other arguments, specific to the engine, that you would like to change or allow to vary. These are accessible using `set_engine`. For example, `ranger` has an option to set the internal random number seed. To set this to a specific value: 

```{r rf-seed}
rf_with_seed <- 
  rand_forest(trees = 2000, mtry = varying(), mode = "regression") %>%
  set_engine("ranger", seed = 63233)
rf_with_seed
```

There are times where you would like to change a parameter from its default but you are not sure what the final value will be. This is the basis for model tuning. Since the model is not executing when created, these types of parameters can be changed using the varying() function. This provides a simple placeholder for the value.

## Process

To fit the model, you must:

* have a defined model, including the _mode_,
* have no `varying()` parameters, and
* specify a computational engine. 

For example, `rf_with_seed` above is not ready for fitting due the `varying()` parameter. We can set that parameter's value and then create the model fit: 

```{r}
rf_with_seed %>% 
  set_args(mtry = 4) %>% 
  set_engine("ranger") %>%
  fit(mpg ~ ., data = mtcars)
```


Or, using the `randomForest` package: 

```{r}
set.seed(56982)
rf_with_seed %>% 
  set_args(mtry = 4) %>% 
  set_engine("randomForest") %>%
  fit(mpg ~ ., data = mtcars)
```


Note that the call objects show `num.trees = ~2000`. The tilde is the consequence of `parsnip` using quosures to process the model specification's arguments. 

Normally, when a function is executed, the function's arguments are immediately evaluated. In the case of `parsnip`, the model specification's arguments are _not_; the expression is captured along with the environment where it should be evaluated. That is what a quosure does. 

`parsnip` uses these expressions to make a model fit call that is evaluated. The tilde in the call above reflects that the argument was captured using a quosure. 

## Model List

```{r modelinfo, include = FALSE}
mod_names <- function(model, engine) {
  obj_name <- paste(model, engine, "data", sep = "_")
  tibble(module = getFromNamespace(obj_name, "parsnip") %>% names(),
         model = model, 
         engine = engine)
}
engine_info <- 
  parsnip:::engine_info %>%
  distinct(model, engine) %>% 
  mutate(obj_name = paste(model, engine, "data", sep = "_")) 
```

`parsnip` contains wrappers for a number of models. For example, the `parsnip` function `rand_forest()` can be used to create a random forest model. The **mode** of a model is related to its goal. Examples would be regression and classification. 

The list of models accessible via `parsnip` is:

```{r model-table, results = 'asis', echo = FALSE}
mod_list <- 
  parsnip:::engine_info %>% 
  distinct(mode, model) %>% 
  mutate(model = paste0("`", model, "()`")) %>%
  arrange(mode, model) %>%
  group_by(mode) %>%
  summarize(models = paste(model, collapse = ", "))
for (i in 1:nrow(mod_list)) {
  cat(mod_list[["mode"]][i], ": ",
      mod_list[["models"]][i], "\n\n\n", 
      sep = "")
}
```

_How_ the model is created is related to the _engine_. In many cases, this is an R modeling package. In others, it may be a connection to an external system (such as Spark or Tensorflow). This table lists the engines for each model type along with the type of prediction that it can make (see `predict.model_fit()`). 

```{r pred-table, results = 'asis', echo = FALSE}
  map2_dfr(engine_info$model, engine_info$engine, mod_names) %>%
  dplyr::filter(!(module %in% c("libs", "fit"))) %>%
  mutate(
    module = ifelse(module == "confint", "conf_int", module),
    module = ifelse(module == "predint", "pred_int", module),   
    module = paste0("`", module, "`"),
    model = paste0("`", model, "()`"),
    ) %>%
  mutate(check = cli::symbol$tick) %>%
  spread(module, check, fill =  cli::symbol$times) %>%
  kable(format = "html") %>% 
  kable_styling(full_width = FALSE) %>%
  collapse_rows(columns = 1)
```

Models can be added by the user too. See the ["Making a `parsnip` model from scratch" vignette](https://tidymodels.github.io/parsnip/articles/articles/Scratch.html).

## Classification Example

We'll use the 'credit_data' for this example.

```{r credit-split}
library(tidymodels)
data(credit_data)
set.seed(7075)
data_split <- initial_split(credit_data, strata = "Status", p = 0.75)
credit_train <- training(data_split)
credit_test  <- testing(data_split)
```

A single layer neural network will be used to predict a person's credit status. To do so, the columns of the predictor matrix should be numeric and on a common scale. `recipes` will be used to get the data in the appropriate form.  

```{r credit-proc}
credit_rec <- 
  recipe(Status ~ ., data = credit_train) %>%
  step_knnimpute(Home, Job, Marital, Income, Assets, Debt) %>%
  step_dummy(all_nominal(), -Status) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  prep(training = credit_train, retain = TRUE)
# juice() will be used to get the processed training set back
test_normalized <- bake(credit_rec, new_data = credit_test, all_predictors())
```

`keras` will be used to fit a model with 5 hidden units and uses a 10% dropout rate to regularize the model. At each training iteration (aka epoch) a random 20% of the data will be used to measure the cross-entropy of the model. 

```{r credit-nnet}
set.seed(57974)
nnet_fit <-
  mlp(epochs = 100, hidden_units = 5, dropout = 0.1) %>%
  # Also set engine-specific arguments: 
  set_engine("keras", verbose = 0, validation_split = .20) %>%
  fit(Status ~ ., data = juice(credit_rec))
nnet_fit
```

In `parsnip`, the `predict` function can be used: 

```{r credit-perf}
test_results <- 
  credit_test %>%
  select(Status) %>%
  as_tibble() %>%
  mutate(
    nnet_class = predict(nnet_fit, new_data = test_normalized) %>% 
      pull(.pred_class),
    nnet_prob  = predict(nnet_fit, new_data = test_normalized, type = "prob") %>% 
      pull(.pred_good)
  )
test_results %>% roc_auc(truth = Status, nnet_prob)
test_results %>% accuracy(truth = Status, nnet_class)
test_results %>% conf_mat(truth = Status, nnet_class)
```

This shows us the performance, accuracy, and the confusion matrix of the model.