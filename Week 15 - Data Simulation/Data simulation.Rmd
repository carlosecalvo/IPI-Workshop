---
title: "Simulating Data in R"
author: "Carlos Calvo Hernandez"
date: "5/15/2019"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
set.seed(123456789)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

# Simulating Data

Simulating data can be a very useful learning tool. It can help each of us better understand the "real world" data we collect by allowing us to mimic the structure of data we hope to collect or data we have collected. It can help us better understand the analyses and models we wish to use, as well as the algorithms upon which those rely.

## Univariate Data Simulation

There are a large number of theoretical distributions which can be simulated using base R. The function `rnorm` creates *random* sampling from a Normal Distribution, given a **mean** and a **standard deviation**.

```{r}
x <- rnorm(n = 10000, mean = 10, sd = 1.5)
hist(x)
mean(x)
sd(x)
```

When no mean or standard deviation are provided, the default values are mean of zero and standard deviation of one.

```{r}
x <- rnorm(10000)
hist(x)
mean(x)
sd(x)
```

There are associated functions for doing the same thing with other theoretical distributions; such as: `rbeta` (Beta distribution), `rexp` (Exponential distribution), `rchisq` (non-central Chi-Squared distribution), `rpois` (Poisson distribution), `rf` (F distribution), `rt` (Student`s t distribution), etc.

Each distribution also has three other functions which provide density, distribution function, and quantile. So, for the Normal distribution(s) there are four standard functions: `rnorm`, `dnorm` (density), `pnorm` (distribution function), and `qnorm` (for quantiles).

## Common Regression-style simulation

We want to demonstrate a couple of commonly used single-outcome prediction/explanation models. First, Ordinary Least Squares, OLS, linear regression. These examples are "perfect" because the assumptions of the models are met: only linear relationships, homoscedasticity of variances, normal distribution of residuals, no multicollinearity, no measurement error, and only the correct variables are included in the model, i.e. no model misspecifications [errors of omission or errors of inclusion].

```{r}
x <- rnorm(100, 10, 1.5)
Zx <- scale(x)
Zy <- .8*Zx + rnorm(100, 0, sqrt(1 - (.8^2)))
```


```{r}
cor(Zx, Zy)
summary(lm(Zy ~ 0 + Zx))
```

We can see that `r cor(Zx, Zy)[1,1]` is very close to the value we chose (0.8) to generate the data.

To see the more familiar non-standardized version, simply use the original 'x' variable as the predictor and apply the same 'scaling' or parameters (i.e. mean & standard deviation) to the outcome, 'y'.

```{r}
y <- (1.5*Zy) + 10
cor(x, y)
summary(lm(y ~ x))
```

We can now add complexity, such as a second predictor and an interaction term.

```{r}
x1 <- rnorm(100, 10, 1.5)
Zx1 <- scale(x1)
x2 <-rnorm(100, 10, 1.5)
Zx2 <- scale(x2)
x3 <- x1*x2
Zx3 <- scale(x3)
Zy <- (.7*Zx1) + (.4*Zx2) + (.2*Zx3) +rnorm(100, 0, sqrt(.31))
y <- (1.5*Zy) + 10
df.1 <- data.frame(x1, x2, x3, y)
rm(x, Zx, Zx1, x1, Zx2, x2, Zx3, x3, Zy, y)
```

Checking the results:

```{r}
summary(lm(y ~ x1 + x2 + x3, data = df.1))

summary(lm(scale(y) ~ 0 + scale(x1) + scale(x2) + scale(x1)*scale(x2), data = df.1))
```


Another common regression model is the binary logistic regression model. We will use the `invlogit` function from the `arm` package.

```{r}
library(arm)
```

Since we are going to be dealing with a binary outcome variable, we use the `rbinom` function.

```{r}
x1 <- rnorm(100)
x2 <- rnorm(100)
x3 <- rnorm(100)
b0 <- 1
b1 <- 1.5
b2 <- 2
b3 <- 0.5
y <- rbinom(100, 1, invlogit(b0 + b1*x1 + b2*x2 + b3*x3))
df.2 <- data.frame(x1, x2, x3, y)
rm(x1, x2, x3, b0, b1, b2, b3, y)
summary(df.2)
df.3 <- df.2
df.3[,4] <- as.factor(df.2[,4])
summary(df.3)
```

We can check both "numeric" and "factor" versions of the outcome variable, y.

```{r}
summary(glm(y ~ x1 + x2 + x3, data = df.2, family = "binomial"))
summary(glm(y ~ x1 + x2 + x3, data = df.3, family = "binomial"))
```

## Simulating mulivariate data structures

The `mvrnorm` functiokn from the `MASS` package produces one or more samples from the specified multivariate normal distribution. This function can be useful for a variety of data structures, such as simulating multicollinear predictor variables in a regression-style model, simulating components, simulating factors, simulating canonical correlations, etc. To demonstrate the function, we create four multivariate normal variables with specified correlations (`Sigma = sig`), specified means (`mu = 0` for all four), and use `empirical = TRUE` to replicate the exact correlations among the simulated variables.

```{r}
sig <- matrix(c(1.0, 0.8, 0.5, 0.2,
                0.8, 1.0, 0.5, 0.5,
                0.5, 0.5, 1.0, 0.5,
                0.2, 0.5, 0.5, 1.0), nrow = 4)
library(MASS)

df.4 <- data.frame(mvrnorm(n = 1000, mu = rep(0, 4), Sigma = sig, empirical = TRUE))

summary(df.4)
ncol(df.4)
nrow(df.4)
```

From here, we can check the correlations of the `sig` matrix versus the correlation matrix of the 1000 cases of the four variables we just simulated:

```{r}
round(sig, 2)
round(cor(df.4), 2)
```

We could then use the first two columns of the data frame as factor scores to recreate a data structure for factor analysis with two related factors (` r = 0.8`).

```{r}
v1 <- .8*df.4[,1] + rnorm(1000, 0, sqrt(1 - (.8^2)))
v2 <- .7*df.4[,1] + rnorm(1000, 0, sqrt(1 - (.7^2)))
v3 <- .6*df.4[,1] + rnorm(1000, 0, sqrt(1 - (.6^2)))
v4 <- .5*df.4[,1] + rnorm(1000, 0, sqrt(1 - (.5^2)))
 
v5 <- .5*df.4[,2] + rnorm(1000, 0, sqrt(1 - (.5^2)))
v6 <- .6*df.4[,2] + rnorm(1000, 0, sqrt(1 - (.6^2)))
v7 <- .7*df.4[,2] + rnorm(1000, 0, sqrt(1 - (.7^2)))
v8 <- .8*df.4[,2] + rnorm(1000, 0, sqrt(1 - (.8^2)))
 
df.5 <- data.frame(v1,v2,v3,v4,v5,v6,v7,v8)
rm(v1,v2,v3,v4,v5,v6,v7,v8)
```

We can check the factor structure (i.e. loadings) with an oblique rotation strategy using the `GPArotation` package:

```{r}
library(GPArotation)
factanal(x = df.5, factors = 2, rotation = "oblimin")
```

Although the above example is nice and useful for gaining some insight into multivariate normal data; it is more common to use other packages. One common package for simulating common specific types of multivariate data structures is `psych`. The `psych` package contains functions for simulating ANOVA / linear models, multilevel models, factor structures (hierarchical models, bi-factor models), simplex and circumplex structures; as well as others. Another common one is the "Latent Variable Analysis" package. The `lavaan` package has functions for simulating data for structural models (e.g., structural equation models [SEM] [^1] ) and the model syntax. 

[^1]: For more on Structural Equation Modeling see: <www.statisticssolutions.com/structural-equation-modeling/>

Let's create a structural model using the `lavaan` package[^2]:

[^2]: Check `?model.syntax` on the `lavaan` package for the particular syntax of the model specification. Some options are: `=~`: latent variable definitions; `~`: regressions; `~~`: variance-covariances; `|`: thresholds; `~*~`: scaling factors; and `<=`: formative factors.

```{r}
library(lavaan)

# Specify the population's structural model with coefficients

sem.model <- '
   f1 =~ x1 + .8*x2 + .6*x3 + .4*x4
  f2 =~ x5 + .8*x6 + .6*x7 + .4*x8
  f3 =~ x9 + .8*x10 + .6*x11 + .4*x12
  f4 =~ x13 + .4*x14 + .6*x15 + .8*x16
  f5 =~ x17 + .6*x18 + .8*x19
  f4 ~ .6*f1
  f3 ~ .8*f2
  f5 ~ .3*f2 + .5*f3
  f1 ~~ 0*f2
  '

```

Now, simulate the data from the specified model.

```{r}
df.6 <- simulateData(model = sem.model, sample.nobs = 1000)
summary(df.6)
```

To check the model, we need to specify so we can then fit that model to the data we just simulated.

```{r}
str.model <- '
  f1 =~ x1 + x2 + x3 + x4
  f2 =~ x5 + x6 + x7 + x8
  f3 =~ x9 + x10 + x11 + x12
  f4 =~ x13 + x14 + x15 + x16
  f5 =~ x17 + x18 + x19
  f4 ~ f1
  f3 ~ f2
  f5 ~ f2 + f3
  f1 ~~ 0*f2
  f1 ~~ f3
  f1 ~~ f5
  f2 ~~ f4
  f3 ~~ f4
  f4 ~~ f5
  '
```

```{r}
# Fitting the model to the data

sem.1 <- sem(model = str.model, data = df.6,
             std.lv = FALSE,
             parameterization = "default",
             std.ov = FALSE,
             ridge = 1e-05,
             estimator = "MLR", likelihood = "default", link = "default",
             information = "default", se = "robust.huber.white")
summary(sem.1, fit.measures = TRUE, standardize = TRUE)
```


